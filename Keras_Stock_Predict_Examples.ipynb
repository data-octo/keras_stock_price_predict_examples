{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度玩转神经网络——基于Keras\n",
    "\n",
    "Keras是基于tensorflow的高级神经网络API，使用Keras也可以搭建各种各样的神经网络。跟其他机器学习算法相比，神经网络可以轻松地实现多输入多输出，可以实现一些比较复杂的传统机器学习模型难以解决的任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example1\n",
    "\n",
    "首先是一个很常规的3层神经网络，我们可以输入股票的基本面的选股因子，输出未来某一段时间股票获得正alpha的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.7072\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6969\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6954\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 0.6950\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6948\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6947\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6945\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6944\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6943\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.6942\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Example1_input (InputLayer)  (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "Example1_dense1 (Dense)      (None, 60)                1860      \n",
      "_________________________________________________________________\n",
      "Example1_dense2 (Dense)      (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "Example1_output (Dense)      (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 5,581\n",
      "Trainable params: 5,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras import layers,Model\n",
    "X = np.random.rand(10000,30) # 10000个样本,30个特征\n",
    "y = np.random.choice([0,1],size=10000)\n",
    "Input = layers.Input(shape=(30,),name='Example1_input')\n",
    "dense1 = layers.Dense(60,activation='relu',name='Example1_dense1')(Input)\n",
    "dense2 = layers.Dense(60,activation='relu',name='Example1_dense2')(dense1)\n",
    "Output = layers.Dense(1,activation='sigmoid',name='Example1_output')(dense2)\n",
    "model = Model(inputs=Input,outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='binary_crossentropy')\n",
    "model.fit(X,y,batch_size=200,epochs=10)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dense1** : 60个节点，每个节点接受30个特征的输入以及自身的偏置值，故有60*(30+1)=1860个参数;\n",
    "\n",
    "**dense2**: 60个节点，每个节点接受dense1的60个输出以及自身的偏置值，故有60*(60+1)=3660个参数;\n",
    "\n",
    "**output**: 1个节点，接受dense2的60个输出以及自身的偏置值，故有1*(60+1)=61个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example2\n",
    "\n",
    "这是一个简单的RNN，很多人把循环神经网络看成是非线性的arima，其实也没错，但是我们使用arima的时候，大多数都是处理单个时间序列，而RNN本质上跟其他机器学习一样，也是用来处理回归和分类问题。例如在情感分析中，输入一段文本，输出文本的情感分类，而训练数据是很多条文本以及文本的标签。在量化投资中，可以输入过去n天各种技术指标的时间序列数据，例如MACD,KDJ,RSI等，输出未来某一段时间股票获得正alpha的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s 313us/step - loss: 1.1577\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 1.1289\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 1.1157\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 3s 272us/step - loss: 1.1095\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 3s 278us/step - loss: 1.1065\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 3s 302us/step - loss: 1.1041\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 1.1029\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 3s 294us/step - loss: 1.1021\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 3s 284us/step - loss: 1.1012\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 3s 299us/step - loss: 1.1005\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Example2_input (InputLayer)  (None, 60, 30)            0         \n",
      "_________________________________________________________________\n",
      "Example2_rnn1 (SimpleRNN)    (None, 60, 50)            4050      \n",
      "_________________________________________________________________\n",
      "Example2_rnn2 (SimpleRNN)    (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Example2_output (Dense)      (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 9,253\n",
      "Trainable params: 9,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(10000,60,30) # 10000个样本,60个时间点,30个特征\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input = layers.Input(shape=(60,30,),name='Example2_input') # 输入60天的30个技术指标\n",
    "rnn1 = layers.SimpleRNN(50,return_sequences=True,name='Example2_rnn1')(Input)\n",
    "rnn2 = layers.SimpleRNN(50,name='Example2_rnn2')(rnn1)\n",
    "Output = layers.Dense(3,activation='softmax',name='Example2_output')(rnn2)\n",
    "model = Model(inputs=Input,outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit(X,y,batch_size=200,epochs=10)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rnn1**: 50个节点，接受输入层30个特征、自身t-1隐层的输出，以及自身的偏置值，故有(30+50+1)*50=4050个参数\n",
    "\n",
    "**rnn2**: 50个节点，接受rnn1的50个隐层的输出、自身t-1隐层的输出，以及自身的偏置值，故有(50+50+1)*50=5050个参数\n",
    "\n",
    "**output**: 3个节点，接受rnn2的50个输出以及自身的偏置值，故有3*(50+1)=153个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example3\n",
    "\n",
    "如果我们认为基本面和技术面不能简单地混合一起用，我们将Example1和Example2结合起来，基本面的特征使用普通的全连接网络，技术面的特征使用时序模型RNN，然后再将它们的输出合并起来，再输入到输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s 333us/step - loss: 1.1238\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 3s 301us/step - loss: 1.1118\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 2s 246us/step - loss: 1.1081\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 1.1054\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 3s 270us/step - loss: 1.1038\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 3s 277us/step - loss: 1.1028\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 1.1017\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 3s 259us/step - loss: 1.1009\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 3s 297us/step - loss: 1.1001\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 1.0999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b58ea90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = np.random.rand(10000,50) # 10000个样本,50个特征\n",
    "X2 = np.random.rand(10000,60,30) # 10000个样本,60个时间点,30个特征\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input1 = layers.Input(shape=(50,),name='Example3_input1')\n",
    "dense1 = layers.Dense(100,activation='relu',name='Example3_dense1')(Input1)\n",
    "dense2 = layers.Dense(100,activation='relu',name='Example3_dense2')(dense1)\n",
    "Input2 = layers.Input(shape=(60,30,),name='Example3_input2')\n",
    "rnn1 = layers.SimpleRNN(60,return_sequences=True,name='Example3_rnn1')(Input2)\n",
    "rnn2 = layers.SimpleRNN(60,name='Example3_rnn2')(rnn1)\n",
    "concat = layers.Concatenate(axis=-1,name='Example3_concat')([dense2,rnn2])\n",
    "Output = layers.Dense(3,activation='softmax',name='output')(concat)\n",
    "model = Model(inputs=[Input1,Input2],outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit([X1,X2],y,batch_size=200,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dense1**: 100个节点，接受input1的50个特征，以及自身的偏置值，故有(50+1)*100=5100个参数;\n",
    "\n",
    "**rnn1**: 50个节点，接受input2的30个特征，以及自身t-1时刻的50个隐层输出，故有(30+50+1)*50=4050个参数;\n",
    "\n",
    "**dense2**: 100个节点，接受dense1的100个隐层输出，以及自身的偏置值，故有(100+1)*100=10100个参数;\n",
    "\n",
    "**rnn2**: 50个节点，接受rnn1的50个隐层的输出、自身t-1隐层的输出，以及自身的偏置值，故有(50+50+1)*50=5050个参数;\n",
    "\n",
    "**output**: 3个节点，接受dense2的100个隐层的输出、rnn2的50个隐层的输出，以及自身的偏置值，故有(100+50+1)*3=453个参数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的结构如下：\n",
    "![example3-workflow.jpeg](images/example3-workflow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example4\n",
    "\n",
    "事实上我们较少使用的简单的RNN，使用LSTM和GRU比较多。另外在搭建网络时，我们一般在每一层网络的输入上做BatchNormalization和Dropout，Example4为Example2的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 15s 2ms/step - loss: 1.7384\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 1.5119\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 1.3897\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 1.3047\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 1.2323\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 1.1961\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 1.1678\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 1.1433\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 1.1278\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 1.1222\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Example4_input (InputLayer)  (None, 60, 30)            0         \n",
      "_________________________________________________________________\n",
      "Example4_BN_input (BatchNorm (None, 60, 30)            120       \n",
      "_________________________________________________________________\n",
      "Example4_Dropout_input (Drop (None, 60, 30)            0         \n",
      "_________________________________________________________________\n",
      "Example4_lstm1 (LSTM)        (None, 60, 50)            16200     \n",
      "_________________________________________________________________\n",
      "Example4_BN_lstm1 (BatchNorm (None, 60, 50)            200       \n",
      "_________________________________________________________________\n",
      "Example4_Dropout_lstm1 (Drop (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "Example4_lstm2 (LSTM)        (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "Example4_BN_lstm2 (BatchNorm (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "Example4_Dropout_lstm2 (Drop (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "Example4_output (Dense)      (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 37,073\n",
      "Trainable params: 36,813\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(10000,60,30) # 10000个样本,60个时间点,30个特征\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input = layers.Input(shape=(60,30,),name='Example4_input')\n",
    "BN_Input = layers.BatchNormalization(name='Example4_BN_input')(Input)\n",
    "Dropout_Input = layers.Dropout(0.5,name='Example4_Dropout_input')(BN_Input)\n",
    "lstm1 = layers.LSTM(50,dropout=0.3,return_sequences=True,name='Example4_lstm1')(Dropout_Input)\n",
    "BN_lstm1 = layers.BatchNormalization(name='Example4_BN_lstm1')(lstm1)\n",
    "Dropout_lstm1 = layers.Dropout(0.5,name='Example4_Dropout_lstm1')(BN_lstm1)\n",
    "lstm2 = layers.LSTM(50,dropout=0.3,name='Example4_lstm2')(Dropout_lstm1)\n",
    "BN_lstm2 = layers.BatchNormalization(name='Example4_BN_lstm2')(lstm2)\n",
    "Dropout_lstm2 = layers.Dropout(0.5,name='Example4_Dropout_lstm2')(BN_lstm2)\n",
    "Output = layers.Dense(3,activation='softmax',name='Example4_output')(Dropout_lstm2)\n",
    "model = Model(inputs=Input,outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit(X,y,batch_size=200,epochs=10)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BN_input**: 接受30个特征输入，每一个特征都需要求出样本的均值，方差，然后要给出γ和β，故有30*2=60个无需训练的参数和30*2=60个需要训练的参数。总共120个参数；\n",
    "\n",
    "**lstm1**: 50个隐层节点，每一个节点接受30个特征输入和50个t-1隐层输出以及偏置值，每一个隐层节点，都对应着输入门，遗忘门和输出门，它们也都有30个特征输入和50个t-1隐层输出以及偏置值，跟rnn相比，多出了三个门的参数，参数的个数变成了原来的4倍。故参数的个数总共有(30+50+1)*50*4=16200;\n",
    "\n",
    "**BN_lstm1**: 接受lstm1的50个隐层节点的输出，故有50*2=100个无需训练的参数和50*2=100个需要训练的参数。总共200个参数;\n",
    "\n",
    "**lstm2**: 50个隐层节点，还有对应的输入门，遗忘门，输出门，都接受lstm1的50个隐层输出、自身50个t-1隐层输出以及偏置值，故参数的个数总共有(50+50+1)*50*4=20200;\n",
    "\n",
    "**BN_lstm2**: 接受lstm2的50个隐层节点的输出，故有50*2=100个无需训练的参数和50*2=100个需要训练的参数。总共200个参数;\n",
    "\n",
    "**output**: 3个节点，接受lstm2的50个隐层节点的输出以及自身的偏置值，故有(50+1)*3=153个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example5\n",
    "\n",
    "将Example4中BatchNormalization和Dropout应用到Example3中，再将Example3中SimpleRNN换成LSTM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 10s 953us/step - loss: 1.9417\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 7s 667us/step - loss: 1.6537\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 5s 538us/step - loss: 1.4843\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 6s 584us/step - loss: 1.3584\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 6s 610us/step - loss: 1.2662\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 6s 588us/step - loss: 1.2028\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 6s 589us/step - loss: 1.1750\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 7s 741us/step - loss: 1.1491\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 6s 624us/step - loss: 1.1308\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 6s 618us/step - loss: 1.1209\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Example5_input1 (InputLayer)    (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Example5_input2 (InputLayer)    (None, 60, 30)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Example5_BN_input1 (BatchNormal (None, 50)           200         Example5_input1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Example5_BN_input2 (BatchNormal (None, 60, 30)       120         Example5_input2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Example5_Dropout_input1 (Dropou (None, 50)           0           Example5_BN_input1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Example5_Dropout_input2 (Dropou (None, 60, 30)       0           Example5_BN_input2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Example5_dense1 (Dense)         (None, 100)          5100        Example5_Dropout_input1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Example5_rnn1 (SimpleRNN)       (None, 60, 60)       5460        Example5_Dropout_input2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Example5_BN_dense1 (BatchNormal (None, 100)          400         Example5_dense1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Example5_BN_rnn1 (BatchNormaliz (None, 60, 60)       240         Example5_rnn1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Example5_Dropout_dense1 (Dropou (None, 100)          0           Example5_BN_dense1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Example5_Dropout_rnn1 (Dropout) (None, 60, 60)       0           Example5_BN_rnn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Example5_dense2 (Dense)         (None, 100)          10100       Example5_Dropout_dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Example5_rnn2 (SimpleRNN)       (None, 60)           7260        Example5_Dropout_rnn1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Example5_concat (Concatenate)   (None, 160)          0           Example5_dense2[0][0]            \n",
      "                                                                 Example5_rnn2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Example5_BN_concat (BatchNormal (None, 160)          640         Example5_concat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Example5_Dropout_concat (Dropou (None, 160)          0           Example5_BN_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 3)            483         Example5_Dropout_concat[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 30,003\n",
      "Trainable params: 29,203\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X1 = np.random.rand(10000,50) # 10000个样本,50个特征\n",
    "X2 = np.random.rand(10000,60,30) # 10000个样本,60个时间点,30个特征\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input1 = layers.Input(shape=(50,),name='Example5_input1')\n",
    "BN_Input1 = layers.BatchNormalization(name='Example5_BN_input1')(Input1)\n",
    "Dropout_Input1 = layers.Dropout(0.5,name='Example5_Dropout_input1')(BN_Input1)\n",
    "dense1 = layers.Dense(100,activation='relu',name='Example5_dense1')(Dropout_Input1)\n",
    "BN_dense1 = layers.BatchNormalization(name='Example5_BN_dense1')(dense1)\n",
    "Dropout_dense1 = layers.Dropout(0.5,name='Example5_Dropout_dense1')(BN_dense1)\n",
    "dense2 = layers.Dense(100,activation='relu',name='Example5_dense2')(Dropout_dense1)\n",
    "Input2 = layers.Input(shape=(60,30,),name='Example5_input2')\n",
    "BN_Input2 = layers.BatchNormalization(name='Example5_BN_input2')(Input2)\n",
    "Dropout_Input2 = layers.Dropout(0.5,name='Example5_Dropout_input2')(BN_Input2)\n",
    "rnn1 = layers.SimpleRNN(60,dropout=0.3,return_sequences=True,name='Example5_rnn1')(Dropout_Input2)\n",
    "BN_rnn1 = layers.BatchNormalization(name='Example5_BN_rnn1')(rnn1)\n",
    "Dropout_rnn1 = layers.Dropout(0.5,name='Example5_Dropout_rnn1')(BN_rnn1)\n",
    "rnn2 = layers.SimpleRNN(60,name='Example5_rnn2')(Dropout_rnn1)\n",
    "concat = layers.Concatenate(axis=-1,name='Example5_concat')([dense2,rnn2])\n",
    "BN_concat = layers.BatchNormalization(name='Example5_BN_concat')(concat)\n",
    "Dropout_concat = layers.Dropout(0.5,name='Example5_Dropout_concat')(BN_concat)\n",
    "Output = layers.Dense(3,activation='softmax',name='output')(Dropout_concat)\n",
    "model = Model(inputs=[Input1,Input2],outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit([X1,X2],y,batch_size=200,epochs=10)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example6\n",
    "\n",
    "一般我们使用全接连网络训练基本面因子，使用LSTM训练技术面指标；如果你想输入最近几个交易日的分钟行情数据，交易日的数据是属于时间序列数据，而在一个交易日中，分钟行情数据又是时间序列数据，那么我们使用如下的网络进行训练。\n",
    "\n",
    "其中TimeDistributed给与了模型一对一，多对多的能力，增加了模型的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 1.0988\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 9s 868us/step - loss: 1.0987\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 1.0986\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 8s 820us/step - loss: 1.0986\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 8s 849us/step - loss: 1.0986\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 9s 878us/step - loss: 1.0986\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 8s 839us/step - loss: 1.0986\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 9s 856us/step - loss: 1.0986\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 9s 865us/step - loss: 1.0986\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 10s 977us/step - loss: 1.0986\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10, 48, 5)         0         \n",
      "_________________________________________________________________\n",
      "Example6_TD1 (TimeDistribute (None, 10, 48, 8)         448       \n",
      "_________________________________________________________________\n",
      "Example6_TD2 (TimeDistribute (None, 10, 8)             544       \n",
      "_________________________________________________________________\n",
      "Example6_lstm1 (LSTM)        (None, 10, 12)            1008      \n",
      "_________________________________________________________________\n",
      "Example6_lstm2 (LSTM)        (None, 6)                 456       \n",
      "_________________________________________________________________\n",
      "Example_output (Dense)       (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 2,477\n",
      "Trainable params: 2,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(10000,10,48,5) #10000个样本,10个交易,48根5分钟k线,5个特征\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input = layers.Input(shape=(10,48,5,))\n",
    "TD1 = layers.TimeDistributed(layers.LSTM(8,return_sequences=True),name='Example6_TD1')(Input)\n",
    "TD2 = layers.TimeDistributed(layers.LSTM(8),name='Example6_TD2')(TD1)\n",
    "lstm1 = layers.LSTM(12,return_sequences=True,name='Example6_lstm1')(TD2)\n",
    "lstm2 = layers.LSTM(6,name='Example6_lstm2')(lstm1)\n",
    "Output = layers.Dense(3,activation='softmax',name='Example_output')(lstm2)\n",
    "model = Model(inputs=Input,outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit(X,y,batch_size=200,epochs=10)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TD1**: 10个交易日均使用同一个lstm，这个lstm有8个隐层节点，输入的时间长度是48，接受5个特征以及t-1隐层的输出，还有一个偏置值，参数的个数是(5+8+1)*8*4=448;\n",
    "\n",
    "**TD2**: 8个隐层节点，TD1中lstm的8个隐层节点的输出，以及自身t-1隐层的输出，还有一个偏置值，参数的个数是(8+8+1)*8*4=544;\n",
    "\n",
    "**lstm1**: 12个节点， 10个交易的TD2均有8个输出，以及自身t-1隐层的输出，还有一个偏置值，故有(8+12+1)*12*4=1008;\n",
    "\n",
    "**lstm2**: 6个节点， lstm1均有12个输出，以及自身t-1隐层的输出，还有一个偏置值，故有(12+6+1)*6*4=456个参数;\n",
    "\n",
    "TD1和TD2中的lstm是应用在时间长度为48的5分钟k线上的，而lstm1和lstm2是应用在10个交易日上的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example7\n",
    "\n",
    "Example7是使用神经网络进行排序学习，假设我们有一组股票比另一组股票更值得买入，那么可以尝试使用排序学习，训练出一个打分模型，帮助我们挑选出更值得买入的股票。这里展示排序学习主要是展示共享网络层的用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 117us/step - loss: 0.6366\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.5383\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.4611\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.4001\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.3511\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.3114\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.2788\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.2517\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.2289\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.2095\n"
     ]
    }
   ],
   "source": [
    "# 假设X1的样本相对于X2对应位置的样本更值得买入\n",
    "X1 = np.random.rand(10000,30)\n",
    "X2 = np.random.rand(10000,30)\n",
    "y = np.ones(10000) # y永远为1\n",
    "# h1和h2是没有给定Input的网络层\n",
    "h1 = layers.Dense(50,activation='relu',name='Example7_h1')\n",
    "h2 = layers.Dense(1,activation='relu',name='Example7_h2')\n",
    "# 定义两个Input层，接收X1和X2\n",
    "Input1 = layers.Input(shape=(30,),name='Example7_input1')\n",
    "Input2 = layers.Input(shape=(30,),name='Example7_input2')\n",
    "#从Input1和Input2进来的数据流经过h1和h2之后，得到Score1和Score2\n",
    "Score1 = h2(h1(Input1))\n",
    "Score2 = h2(h1(Input2))\n",
    "# 将Score1和Score2相减，得到得分差\n",
    "Subtract = layers.Subtract(name='Example7_Subtract')([Score1,Score2])\n",
    "# 每一个Score1都尽可能要大于Score2，故Output应尽可能输出1\n",
    "Output = layers.Dense(1,activation='sigmoid',name='Example7_output')(Subtract)\n",
    "model = Model(inputs=[Input1,Input2],outputs=Output)\n",
    "model.compile(optimizer='sgd',loss='binary_crossentropy')\n",
    "model.fit([X1,X2],y,batch_size=200,epochs=10)\n",
    "from keras import backend\n",
    "#定义Input1到Score1的数据流路径，这里使用Input1到Score1也是一样的\n",
    "get_score = backend.function([Input1], [Score1]) #定义Input1到Score1的数据流路径，这里\n",
    "X = np.vstack((X1,X2))\n",
    "Score = get_score([X])[0] #Score进行选股即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example8\n",
    "\n",
    "Example8为多目标学习，假设你想使用基本面因子对股票做未来一个季度或者一个月的收益预测，然后使用技术面因子对股票做未来5个交易日或者10个交易日的收益预测，那么可以使用多目标学习，即可以输入多个X，输出多个Y。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 3.2444 - Example8_output1_loss: 1.1084 - Example8_output2_loss: 1.0276\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 3.2241 - Example8_output1_loss: 1.1046 - Example8_output2_loss: 1.0149\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 3.2178 - Example8_output1_loss: 1.1031 - Example8_output2_loss: 1.0116\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 3.2124 - Example8_output1_loss: 1.1020 - Example8_output2_loss: 1.0083\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 3.2092 - Example8_output1_loss: 1.1012 - Example8_output2_loss: 1.0068\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 3.2064 - Example8_output1_loss: 1.1006 - Example8_output2_loss: 1.0051\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 3.2046 - Example8_output1_loss: 1.1002 - Example8_output2_loss: 1.0042\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 3.2029 - Example8_output1_loss: 1.0999 - Example8_output2_loss: 1.0030\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 3.2013 - Example8_output1_loss: 1.0996 - Example8_output2_loss: 1.0021\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 3.1996 - Example8_output1_loss: 1.0993 - Example8_output2_loss: 1.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12bc17630>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = np.random.rand(10000,30) #假设为基本面因子\n",
    "X2 = np.random.rand(10000,50) #假设为技术面因子\n",
    "# 假设y1为分类任务，y2为回归任务\n",
    "y1 = np.random.choice([0,1,2],size=10000)\n",
    "y1 = np.eye(3,dtype=int)[y1]\n",
    "y2 = np.random.randn(10000)\n",
    "Input1 = layers.Input(shape=(30,),name='Example8_input1')\n",
    "dense1 = layers.Dense(60,activation='relu',name='Example8_dense1')(Input1)\n",
    "dense2 = layers.Dense(20,activation='relu',name='Example8_dense2')(dense1)\n",
    "Output1 = layers.Dense(3,activation='softmax',name='Example8_output1')(dense2)\n",
    "Input2 = layers.Input(shape=(50,),name='Example8_input2')\n",
    "Concat = layers.Concatenate(axis=-1,name='Example8_concat')([Input2,dense2])\n",
    "dense3 = layers.Dense(80,activation='relu',name='Example8_dense3')(Concat)\n",
    "dense4 = layers.Dense(50,activation='relu',name='Example8_dense4')(dense3)\n",
    "Output2 =layers.Dense(1,name='Example8_output2')(dense4)\n",
    "model = Model(inputs=[Input1,Input2],outputs=[Output1,Output2])\n",
    "# 分类任务用交叉熵损失函数，回归任务用均方误差损失函数，回归和分类的损失权重假设为2:1\n",
    "model.compile(optimizer='sgd',\n",
    " loss={'Example8_output1':'categorical_crossentropy','Example8_output2':'mse'},\n",
    " loss_weights = {'Example8_output1':2,'Example8_output2':1})\n",
    "model.fit([X1,X2],[y1,y2],batch_size=200,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络结构如下：\n",
    "![example8-workflow.jpeg](images/example8-workflow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example9\n",
    "\n",
    "下面再展示CNN的例子，CNN一般是用来做图像识别的，这里我们利用CNN来捕捉价格突变的信息，但是在逻辑上能使用1维卷积。这里先示范一个简单的CNN的例子，我们输入一天中的分钟价格序列，输出某一个我们自定义的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/USB-128G/Projects/test/venv/python3/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Ex..., outputs=Tensor(\"Ex...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 2s 223us/step - loss: 1.1104\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 112us/step - loss: 1.1074\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 113us/step - loss: 1.1069\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 109us/step - loss: 1.1047\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 129us/step - loss: 1.1043\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 1.1034\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 1.1023\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 1.1019\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 1.1010\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 1.1003\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Example9_input (InputLayer)  (None, 240, 1)            0         \n",
      "_________________________________________________________________\n",
      "Example_9_Conv1 (Conv1D)     (None, 240, 5)            25        \n",
      "_________________________________________________________________\n",
      "Example9_maxpool (MaxPooling (None, 240, 5)            0         \n",
      "_________________________________________________________________\n",
      "Example_9_Conv2 (Conv1D)     (None, 237, 5)            105       \n",
      "_________________________________________________________________\n",
      "Example9_avgpool (AveragePoo (None, 234, 5)            0         \n",
      "_________________________________________________________________\n",
      "Example9_flatten (Flatten)   (None, 1170)              0         \n",
      "_________________________________________________________________\n",
      "Example9_output (Dense)      (None, 3)                 3513      \n",
      "=================================================================\n",
      "Total params: 3,643\n",
      "Trainable params: 3,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(10000,240,1) # 10000个样本，240根分钟k线的价格\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input = layers.Input(shape=(240,1,),name='Example9_input')\n",
    "Conv1 = layers.Conv1D(filters=5,kernel_size=4,strides=1,padding='same',name='Example_9_Conv1')(Input)\n",
    "maxpool = layers.MaxPool1D(pool_size=4,strides=1,padding='same',name='Example9_maxpool')(Conv1)\n",
    "Conv2 = layers.Conv1D(filters=5,kernel_size=4,strides=1,padding='valid',name='Example_9_Conv2')(maxpool)\n",
    "avgpool = layers.AveragePooling1D(pool_size=4,strides=1,padding='valid',name='Example9_avgpool')(Conv2)\n",
    "flatten = layers.Flatten(name='Example9_flatten')(avgpool)\n",
    "Output = layers.Dense(3,activation='softmax',name='Example9_output')(flatten)\n",
    "model = Model(inputs=Input,output=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit(X,y,batch_size=200,epochs=10)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conv1**: filters个数为5, kernel_size为4，可以认为输入的数据filters为1，还有一个偏置值，故参数的个数为{ (kernel_size * 输入数据的filters + 偏置值个数) * filters个数 }=(4*1+1)*5=25，因为padding指定为'same'，所以shape为(240,5)；\n",
    "\n",
    "**maxpool**: padding='same'，shape不变;\n",
    "\n",
    "**Conv2**: filters个数为5，kernel_size为4，因为上一层的filters为5，故参数的个数为(4*5+1)*5=105，因为padding指定为'valid'，故大小将减小(kernel_size-1)，shape由(240,5)变成了(237,5)；\n",
    "\n",
    "**avgpool**: padding='valid'，shape由(237,5)变成(234,5);\n",
    "\n",
    "**output**: 接受经过flatten的234*5=1170个输出，以及偏置值，故有(1170+1)*3=3513个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example10\n",
    "\n",
    "将Example9的CNN替换Example6中分钟数据的lstm，分钟数据采用CNN处理，然后每个交易日都得到输出之后，再输入到LSTM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/USB-128G/Projects/test/venv/python3/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Ex..., outputs=Tensor(\"Ex...)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 1.0995\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 1.0994\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 17s 2ms/step - loss: 1.0993\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 1.0991\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 1.0989\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 1.0987\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 1.0990\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 17s 2ms/step - loss: 1.0989\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 17s 2ms/step - loss: 1.0988\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 17s 2ms/step - loss: 1.0989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b9b8240>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(10000,10,240,1) #10000个样本，10个交易日,240根分钟k线价格数据\n",
    "y = np.random.choice([0,1,2],size=10000)\n",
    "y = np.eye(3,dtype=int)[y]\n",
    "Input = layers.Input(shape=(10,240,1,),name='Example10_input')\n",
    "TD_Conv1 = layers.TimeDistributed(layers.Conv1D(filters=5,kernel_size=4,strides=1,padding='same'),name='Example10_TD_Conv1')(Input)\n",
    "TD_maxpool = layers.TimeDistributed(layers.MaxPool1D(pool_size=4,strides=1,padding='same'),name='Example10_TD_maxpool')(TD_Conv1)\n",
    "TD_Conv2 = layers.TimeDistributed(layers.Conv1D(filters=5,kernel_size=4,strides=1),name='Example10_TD_Conv2')(TD_maxpool)\n",
    "TD_avgpool = layers.TimeDistributed(layers.AveragePooling1D(pool_size=4,strides=1),name='Example10_TD_avgpool')(TD_Conv2)\n",
    "TD_flatten = layers.TimeDistributed(layers.Flatten(),name='Example10_TD_flatten')(TD_avgpool)\n",
    "lstm1 = layers.LSTM(50,name='Exammple10_lstm1',return_sequences=True)(TD_flatten)\n",
    "lstm2 = layers.LSTM(30,name='Exammple10_lstm2')(lstm1)\n",
    "Output = layers.Dense(3,activation='softmax',name='Example10_output')(lstm2)\n",
    "model = Model(inputs=Input,output=Output)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy')\n",
    "model.fit(X,y,batch_size=200,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合Example6和Example9即可将参数的个数以及每一层的Output shape算清楚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "Keras中文文档： https://keras.io/zh/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
